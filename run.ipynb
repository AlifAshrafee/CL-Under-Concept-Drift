{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # needed (don't change it)\n",
    "import importlib\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "import setproctitle\n",
    "import torch\n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from datasets import NAMES as DATASET_NAMES  # nopep8\n",
    "from datasets import ContinualDataset, get_dataset  # nopep8\n",
    "from models import get_all_models, get_model  # nopep8\n",
    "\n",
    "from utils.args import add_management_args\n",
    "from utils.best_args import best_args\n",
    "from utils.conf import set_random_seed\n",
    "from utils.continual_training import train as ctrain\n",
    "from utils.distributed import make_dp\n",
    "from utils.training import train\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "\n",
    "seed_value = 0\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    seed=0, \n",
    "    notes=None, \n",
    "    non_verbose=1, \n",
    "    disable_log=0, \n",
    "    validation=0, \n",
    "    ignore_other_metrics=1, \n",
    "    debug_mode=0, \n",
    "    nowand=1, \n",
    "    wandb_entity='regaz', \n",
    "    wandb_project='mammoth', \n",
    "    dataset='seq-cifar10', \n",
    "    model='er', \n",
    "    lr=0.1, \n",
    "    optim_wd=0.0, \n",
    "    optim_mom=0.0, \n",
    "    optim_nesterov=0, \n",
    "    n_epochs=50, \n",
    "    batch_size=32, \n",
    "    distributed='dp', \n",
    "    poisoning_type=1, \n",
    "    poisoning_severity=5, \n",
    "    n_slots=None, \n",
    "    n_poisonings=1, \n",
    "    max_classes_per_poisoning=0, \n",
    "    sequential_poisonings=False, \n",
    "    buffer_size=5000, \n",
    "    minibatch_size=32, \n",
    "    buffer_mode='reservoir', \n",
    "    conf_jobnum='c41d7384-6e52-4d1d-8799-7683921336e2', \n",
    "    conf_timestamp='2024-10-23 22:05:53.528238', \n",
    "    conf_host='DESKTOP-7A'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Task 1:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 2:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 3:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 4:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 5:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "First task train data shape: (10000, 32, 32, 3)\n",
      "First task test data shape: (2000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download and transform CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Convert dataset from Tensor to NumPy format\n",
    "train_data = np.array(trainset.data)  # CIFAR10 dataset's 'data' is already NumPy\n",
    "train_labels = np.array(trainset.targets)\n",
    "\n",
    "test_data = np.array(testset.data)\n",
    "test_labels = np.array(testset.targets)\n",
    "\n",
    "# Function to split the dataset based on class labels\n",
    "def get_class_indices(labels, class_range):\n",
    "    \"\"\"Return indices in the dataset that belong to the specified class range.\"\"\"\n",
    "    indices = [i for i, label in enumerate(labels) if label in class_range]\n",
    "    return indices\n",
    "\n",
    "def create_task_datasets(data, labels, task_classes):\n",
    "    \"\"\"Create task datasets split by class and return the split data and labels.\"\"\"\n",
    "    task_datasets = []\n",
    "    for class_range in task_classes:\n",
    "        indices = get_class_indices(labels, class_range)\n",
    "        task_data = data[indices]\n",
    "        task_labels = labels[indices]\n",
    "        task_datasets.append({'data': task_data, 'labels': task_labels})\n",
    "    return task_datasets\n",
    "\n",
    "# Define the class ranges for each task\n",
    "task_classes = [\n",
    "    [0, 1],\n",
    "    [2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7],\n",
    "    [8, 9]\n",
    "]\n",
    "\n",
    "# Create task datasets for both trainset and testset\n",
    "train_task_datasets = create_task_datasets(train_data, train_labels, task_classes)\n",
    "test_task_datasets = create_task_datasets(test_data, test_labels, task_classes)\n",
    "\n",
    "# Verify each task's dataset size\n",
    "for i, (train_task, test_task) in enumerate(zip(train_task_datasets, test_task_datasets)):\n",
    "    print(f\"Task {i + 1}:\")\n",
    "    print(f\"Trainset size: {train_task['data'].shape[0]} | Testset size: {test_task['data'].shape[0]}\")\n",
    "\n",
    "# Example: Access data and labels for the first train and test task\n",
    "train_task_data, train_task_labels = train_task_datasets[0]['data'], train_task_datasets[0]['labels']\n",
    "test_task_data, test_task_labels = test_task_datasets[0]['data'], test_task_datasets[0]['labels']\n",
    "\n",
    "print(\"First task train data shape:\", train_task_data.shape)\n",
    "print(\"First task test data shape:\", test_task_data.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[2 3]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[4 5]\n",
      "[6 7]\n",
      "[6 7]\n",
      "[8 9]\n",
      "[8 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    print(np.unique(train_task_datasets[i]['labels']))\n",
    "    print(np.unique(test_task_datasets[i]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft2, fftshift, ifftshift, ifft2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class dataset_transform(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "        self.transform = transform  # save the transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)#self.x.shape[0]  # return 1 as we have only one image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the augmented image\n",
    "        if self.transform:\n",
    "            x = self.transform(self.x[idx])\n",
    "        else:\n",
    "            x = self.x[idx]\n",
    "\n",
    "        return x.float(), self.y[idx]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, extra_labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = torch.from_numpy(labels).type(torch.LongTensor)\n",
    "        self.origin = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        original_x = self.origin[index]\n",
    "\n",
    "        # Convert y and extra_y to LongTensor if they are numpy arrays\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, original_x\n",
    "\n",
    "def poison_transform(img, poisoning_strategy=0, factor=0.5):\n",
    "    # Skip poisoning if random factor condition is not met\n",
    "    if random.random() > factor:\n",
    "        return img\n",
    "    \n",
    "    if poisoning_strategy == 0:  # Random noise\n",
    "        noise = np.random.randn(*img.shape) * 0.2\n",
    "        return img + noise\n",
    "\n",
    "    elif poisoning_strategy == 1:  # Pixel permutation\n",
    "        img_flat = img.flatten()\n",
    "        np.random.shuffle(img_flat)  # Shuffle pixels randomly\n",
    "        return img_flat.reshape(img.shape)\n",
    "\n",
    "    elif poisoning_strategy == 2:  # Low frequencies\n",
    "        return apply_frequency_filter(img, mode='low')\n",
    "\n",
    "    elif poisoning_strategy == 3:  # High frequencies\n",
    "        return apply_frequency_filter(img, mode='high')\n",
    "\n",
    "    elif poisoning_strategy == 4:  # Add square\n",
    "        return add_square(img)\n",
    "\n",
    "    elif poisoning_strategy == 5:  # Vertical flip\n",
    "        return np.flipud(img)  # Flip image vertically\n",
    "\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def apply_frequency_filter(img, mode='low'):\n",
    "    # Apply Fourier transform and shift frequencies\n",
    "    img_fft = fft2(img)\n",
    "    img_fft_shifted = fftshift(img_fft)\n",
    "\n",
    "    rows, cols = img.shape[-2], img.shape[-1]\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "    mask = np.zeros((rows, cols))\n",
    "\n",
    "    # Create a mask for low or high frequencies\n",
    "    if mode == 'low':\n",
    "        mask[crow - 30:crow + 30, ccol - 30:ccol + 30] = 1\n",
    "    else:  # High frequencies\n",
    "        mask[:crow - 30, :] = 1\n",
    "        mask[crow + 30:, :] = 1\n",
    "        mask[:, :ccol - 30] = 1\n",
    "        mask[:, ccol + 30:] = 1\n",
    "\n",
    "    # Apply the frequency filter and inverse transform\n",
    "    img_fft_filtered = img_fft_shifted * mask\n",
    "    img_fft_ishifted = ifftshift(img_fft_filtered)\n",
    "    img_filtered = ifft2(img_fft_ishifted).real\n",
    "\n",
    "    return img_filtered\n",
    "\n",
    "def add_square(img):\n",
    "    # Add a random square patch with random color\n",
    "    rows, cols = img.shape[-2], img.shape[-1]\n",
    "    \n",
    "    # Ensure square size doesn't exceed image size\n",
    "    if rows < 5 or cols < 5:\n",
    "        return img  # Skip if the image is too small for a square\n",
    "    \n",
    "    square_size = random.randint(5, min(10, rows, cols))  # Dynamically adjust square size\n",
    "    color = random.random()\n",
    "\n",
    "    start_row = random.randint(0, rows - square_size)\n",
    "    start_col = random.randint(0, cols - square_size)\n",
    "\n",
    "    # Set the square patch to a random color\n",
    "    img[start_row:start_row + square_size, start_col:start_col + square_size] = color\n",
    "    return img\n",
    "\n",
    "def apply_poisoning(x_train, poisoning_strategy=0, factor=1.0):\n",
    "    # Apply poisoning strategy to each image in the dataset using vectorized operation\n",
    "    return np.array([poison_transform(img, poisoning_strategy=poisoning_strategy, factor=factor) for img in x_train])\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "\n",
    "    def avg(self):\n",
    "        if self.count == 0:\n",
    "            return 0\n",
    "        return float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:46<05:41,  7.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m loss_meter \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m     53\u001b[0m acc_meter \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     55\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmeta_observe(inputs, labels, inputs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\anaconda\\envs\\poison\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from backbone.ResNet18 import resnet18\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "backbone = resnet18(10)\n",
    "loss = F.cross_entropy\n",
    "\n",
    "model = get_model(args, backbone, loss,  transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), TRANSFORM]))\n",
    "# model = get_model(args, backbone, loss,  transform = transforms.Compose(\n",
    "#             [TRANSFORM]))\n",
    "print(model.__class__.__name__)\n",
    "# Set a seed for reproducibility\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "## train process\n",
    "for task, train_data in enumerate(train_task_datasets):\n",
    "    model.net.train()\n",
    "    x_train, y_train = train_data['data'], train_data['labels']\n",
    "    ##################poisoning#############################\n",
    "    \n",
    "    if task ==2:\n",
    "        poisoning_strategy ={0: 'Random noise', \n",
    "                            1: 'Pixel permutation', \n",
    "                            2: 'Low frequencies', \n",
    "                            3: 'High frequencies', \n",
    "                            4: 'Add square', \n",
    "                            5: 'Vertical flip'}\n",
    "        print('poisoning the data with {}'.format(poisoning_strategy[0]))\n",
    "        x_train_poison = []\n",
    "        for x in x_train:\n",
    "            x_poison = poison_transform(x, poisoning_strategy=0, factor=0.5)\n",
    "            x_train_poison.append(x_poison)\n",
    "        x_train_poison = np.array(x_train_poison)\n",
    "        x_train = x_train_poison\n",
    "    #####################################################\n",
    "    for epoch in tqdm.tqdm(range(50)):\n",
    "        train_dataset = dataset_transform(x_train, y_train, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ]))\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "        model.net.train()\n",
    "        loss_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            loss = model.meta_observe(inputs, labels, inputs)\n",
    "\n",
    "    # if task == 2:\n",
    "    #     break\n",
    "        \n",
    "    #eval\n",
    "    model.net.eval()\n",
    "    acc_array = np.zeros(len(test_task_datasets))\n",
    "    with torch.no_grad():\n",
    "        for task, test_data in enumerate(test_task_datasets):\n",
    "            acc = AverageMeter()\n",
    "            x_test, y_test = test_data['data'], test_data['labels']\n",
    "            test_dataset = dataset_transform(x_test, y_test, transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "            ]))\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=500, shuffle=False)\n",
    "            # acc_meter = AverageMeter()\n",
    "            for i, (inputs, labels) in enumerate(test_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model.net(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_cnt = (preds == labels).sum().item() / inputs.size(0) \n",
    "\n",
    "                acc.update(correct_cnt, inputs.size(0))\n",
    "            acc_array[task] = acc.avg()\n",
    "    print(acc_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.FloatTensor(50).uniform_(0, 100).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 44,  6, 91, 33, 71, 38, 50, 89, 45, 81, 60, 57, 36, 62, 43, 20, 52,\n",
      "        11, 13, 94, 64, 93, 86, 94, 90,  6, 19, 80, 18,  4, 78, 86, 30,  1, 59,\n",
      "        30, 32,  5, 48, 60,  7, 32, 90, 43,  9,  1, 88, 96, 31])\n"
     ]
    }
   ],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "place_left = max(0, 50)\n",
    "print(place_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 16,  7, 16, 15])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.FloatTensor(5).uniform_(0, 20).long()\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0])\n",
      "tensor([2])\n",
      "tensor([7])\n",
      "{7: 2}\n"
     ]
    }
   ],
   "source": [
    "valid_indices = (indices < 10).long()\n",
    "print(valid_indices)\n",
    "idx_new_data = valid_indices.nonzero().squeeze(-1)\n",
    "idx_buffer   = indices[idx_new_data]\n",
    "print(idx_new_data)\n",
    "print(idx_buffer)\n",
    "idx_map = {idx_buffer[i].item(): idx_new_data[i].item() for i in range(idx_buffer.size(0))}\n",
    "print(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
